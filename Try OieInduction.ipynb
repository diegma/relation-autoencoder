{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32mdata-sample.txt\u001b[0m*  __init__.py   main.py      sample.pk  Untitled.ipynb\r\n",
      "\u001b[01;34mdefinitions\u001b[0m/      __init__.pyc  main.pyc     tags\r\n",
      "dev.pk            \u001b[01;34mlearning\u001b[0m/     \u001b[01;34mprocessing\u001b[0m/  test.pk\r\n",
      "\u001b[01;34mevaluation\u001b[0m/       LICENSE.txt   README.md    train.pk\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python learning/OieInduction.py --pickled_dataset sample.pk --model_name discrete-autoencoder --model AC --optimization 1 --epochs 10 --batch_size 100 --relations_number 10 --negative_samples_number 5 --l2_regularization 0.1 --alpha 0.1 --seed 2 --embed_size 10 --learning_rate 0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "help funcs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_dict(tmp_dict):\n",
    "    for item in tmp_dict:\n",
    "        print \"{:30s} : {:10s}\".format(item, str(tmp_dict[item]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '/home/yliangav/anaconda3/envs/py27/lib/python27.zip', '/home/yliangav/anaconda3/envs/py27/lib/python2.7', '/home/yliangav/anaconda3/envs/py27/lib/python2.7/plat-linux2', '/home/yliangav/anaconda3/envs/py27/lib/python2.7/lib-tk', '/home/yliangav/anaconda3/envs/py27/lib/python2.7/lib-old', '/home/yliangav/anaconda3/envs/py27/lib/python2.7/lib-dynload', '/home/yliangav/anaconda3/envs/py27/lib/python2.7/site-packages', '/home/yliangav/anaconda3/envs/py27/lib/python2.7/site-packages/Sphinx-1.5.6-py2.7.egg', '/home/yliangav/anaconda3/envs/py27/lib/python2.7/site-packages/setuptools-27.2.0-py2.7.egg', '/home/yliangav/anaconda3/envs/py27/lib/python2.7/site-packages/IPython/extensions', '/home/yliangav/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import cPickle as pickle\n",
    "import operator\n",
    "from theano import sparse\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from learning.OieModel import OieModelFunctions\n",
    "\n",
    "from learning.OieData import DataSetManager\n",
    "from learning.OieData import MatrixDataSet\n",
    "from processing.OiePreprocessor import FeatureLexicon\n",
    "from evaluation.OieEvaluation import singleLabelClusterEvaluation\n",
    "import definitions.settings as settings\n",
    "from learning.NegativeExampleGenerator import NegativeExampleGenerator\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from learning.OieInduction import loadData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " dataset format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file must be tab-separated an with the following fields:\n",
    "\n",
    " - lexicalized dependency path between arguments (entities) of the relation,  \n",
    " - first entity  \n",
    " - second entity  \n",
    " - entity types of the first and second entity  \n",
    " - trigger word  \n",
    " - id of the sentence  \n",
    " - raw sentence  \n",
    " - pos tags of the entire sentence  \n",
    " - relation between the two entities if any (used only for evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset pickle file format:\n",
    "```python\n",
    "pickle.dump(featureExtrs, pklFile, protocol=pklProtocol)\n",
    "\n",
    "pickle.dump(relationLexicon, pklFile, protocol=pklProtocol)\n",
    "\n",
    "pickle.dump(dataset, pklFile, protocol=pklProtocol)\n",
    "\n",
    "pickle.dump(goldstandard, pklFile, protocol=pklProtocol)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFeatures(lexicon, featureExs, info, arg1=None, arg2=None, expand=False):\n",
    "    \"\"\"\n",
    "    getFeatures(relationLexicon, featureExtrs, [re[1], re[4], re[5], re[7], re[8], re[6]],\n",
    "                                                             re[2], re[3], True)\n",
    "    :param lexicon: FeatureLexicon instance, init empty(default constructor) \n",
    "    :param featureExs: feature exactors\n",
    "    :param info: features except entity\n",
    "    :param arg1: entity1\n",
    "    :param arg2: entity2\n",
    "    :param expand: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    feats = []\n",
    "    for f in featureExs:\n",
    "        res = f(info, arg1, arg2)\n",
    "        if res is not None:\n",
    "            if type(res) == list:\n",
    "                for el in res:\n",
    "                    featStrId = f.__name__ + \"#\" + el\n",
    "                    if expand:\n",
    "                        feats.append(lexicon.getOrAdd(featStrId))\n",
    "                    else:\n",
    "                        featId = lexicon.getId(featStrId)\n",
    "                        if featId is not None:\n",
    "                            feats.append(featId)\n",
    "            else:\n",
    "                featStrId = f.__name__ + \"#\" + res\n",
    "                if expand:\n",
    "                    feats.append(lexicon.getOrAdd(featStrId))\n",
    "                else:\n",
    "                    featId = lexicon.getId(featStrId)\n",
    "                    if featId is not None:\n",
    "                        feats.append(featId)\n",
    "\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "if args.features == \"basic\":\n",
    "    print \"Using rich features\"\n",
    "```\n",
    "rich(`\"basic\"`, seems the only choice) feature format:\n",
    "```python\n",
    "features = [trigger, entityTypes, arg1_lower, arg2_lower, bow_clean, entity1Type, entity2Type, lexicalPattern,\n",
    "                posPatternPath]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate and print args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getCommandArgs():\n",
    "    parser = argparse.ArgumentParser(description='Trains a basic Open Information Extraction Model')\n",
    "\n",
    "    parser.add_argument('--pickled_dataset', metavar='pickled_dataset', nargs='?', required=True,\n",
    "                        help='the pickled dataset file (produced by OiePreprocessor.py)')\n",
    "\n",
    "    parser.add_argument('--epochs', metavar='epochs', nargs='?', type=int, default=100,\n",
    "                        help='maximum number of epochs')\n",
    "\n",
    "    parser.add_argument('--learning_rate', metavar='learning_rate', nargs='?', type=float, default=0.1,\n",
    "                        help='initial learning rate')\n",
    "\n",
    "    parser.add_argument('--batch_size', metavar='batch_size', nargs='?', type=int, default=50,\n",
    "                        help='size of the minibatches')\n",
    "\n",
    "    parser.add_argument('--embed_size', metavar='embed_size', nargs='?', type=int, default=30,\n",
    "                        help='initial learning rate')\n",
    "\n",
    "    parser.add_argument('--relations_number', metavar='relations_number', type=int, nargs='?', default=3,\n",
    "                        help='number of relations to induce')\n",
    "\n",
    "    parser.add_argument('--negative_samples_number', metavar='negative_samples_number', nargs='?', type=int, default=5,\n",
    "                        help='number of negative samples')\n",
    "\n",
    "    parser.add_argument('--l1_regularization', metavar='l1_regularization', nargs='?', type=float, default=0.0,\n",
    "                        help='lambda value of L1 regulatization')\n",
    "\n",
    "    parser.add_argument('--l2_regularization', metavar='l2_regularization', nargs='?', type=float, default=0.0,\n",
    "                        help='lambda value of L2 regulatization')\n",
    "\n",
    "    parser.add_argument('--optimization', metavar='optimization', nargs='?', type=int, default='0',\n",
    "                        help='optimization algorithm 0 SGD, 1 ADAGrad, 2 ADADelta. Default SDG.')\n",
    "\n",
    "    parser.add_argument('--model_name', metavar='model_name', nargs='?', required=True, type=str,\n",
    "                        help='Name or ID of the model')\n",
    "\n",
    "    parser.add_argument('--model', metavar='model', nargs='?', type=str, required=True,\n",
    "                        help='Model Type choose among A, C, AC.')\n",
    "\n",
    "    parser.add_argument('--fixed_sampling', metavar='fixed_sampling', nargs='?', default='False',\n",
    "                        help='fixed/dynamic sampling switch, default fixed sampling')\n",
    "\n",
    "    parser.add_argument('--ext_emb', metavar='ext_emb', nargs='?', default='False',\n",
    "                        help='external embeddings, default False')\n",
    "\n",
    "    parser.add_argument('--extended_reg', metavar='extended_reg', nargs='?', default='False',\n",
    "                        help='extended regularization on reconstruction parameters, default False')\n",
    "\n",
    "    parser.add_argument('--frequent_eval', metavar='frequent_eval', nargs='?', default='False',\n",
    "                        help='using frequent evaluation, default False')\n",
    "\n",
    "    parser.add_argument('--seed', metavar='seed', nargs='?', type=int, default=2,\n",
    "                        help='random seed, default 2')\n",
    "\n",
    "    parser.add_argument('--alpha', metavar='alpha', nargs='?', type=float, default=1.0,\n",
    "                        help='alpha coefficient for scaling the entropy term')\n",
    "\n",
    "\n",
    "    return parser.parse_args(arg_str.split(' '))\n",
    "\n",
    "def print_args(args):\n",
    "    tmp_dict = vars(args)\n",
    "    for item in tmp_dict:\n",
    "        print \"{:30s} : {:10s}\".format(item, str(tmp_dict[item]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arg_str ='--pickled_dataset train.pk --model_name discrete-autoencoder \\\n",
    "--model AC --optimization 1 --epochs 10 --batch_size 100 --relations_number 10 \\\n",
    "--negative_samples_number 5 --l2_regularization 0.1 --alpha 0.1 --seed 2 --embed_size 10 --learning_rate 0.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative_samples_number        : 5         \n",
      "frequent_eval                  : False     \n",
      "embed_size                     : 10        \n",
      "extended_reg                   : False     \n",
      "l2_regularization              : 0.1       \n",
      "learning_rate                  : 0.1       \n",
      "fixed_sampling                 : False     \n",
      "batch_size                     : 100       \n",
      "model_name                     : discrete-autoencoder\n",
      "epochs                         : 10        \n",
      "optimization                   : 1         \n",
      "alpha                          : 0.1       \n",
      "seed                           : 2         \n",
      "relations_number               : 10        \n",
      "model                          : AC        \n",
      "ext_emb                        : False     \n",
      "pickled_dataset                : train.pk  \n",
      "l1_regularization              : 0.0       \n"
     ]
    }
   ],
   "source": [
    "args = getCommandArgs()\n",
    "print_args(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translate arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing pickled dataset, loading... Done (0.0459129810333s.)\n",
      "Produced indexed dataset\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rseed = args.seed\n",
    "rand = np.random.RandomState(seed=rseed)\n",
    "\n",
    "\n",
    "negativeSamples = args.negative_samples_number\n",
    "numberRelations = args.relations_number\n",
    "indexedData, goldStandard = loadData(args, rand, negativeSamples, numberRelations, args.model)\n",
    "\n",
    "\n",
    "maxEpochs = args.epochs\n",
    "learningRate = args.learning_rate\n",
    "batchSize = args.batch_size\n",
    "embedSize = args.embed_size\n",
    "lambdaL1 = args.l1_regularization\n",
    "lambdaL2 = args.l2_regularization\n",
    "optimization = args.optimization\n",
    "modelName = args.model_name\n",
    "model = args.model\n",
    "fixedSampling = eval(args.fixed_sampling)\n",
    "extEmb = eval(args.ext_emb)\n",
    "extendedReg = eval(args.extended_reg)\n",
    "frequentEval = eval(args.frequent_eval)\n",
    "alpha = args.alpha\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/business/person/company']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goldStandard['train'][4] # goldStandard is the relation between the two entities if any (used only for evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id2Arg',\n",
       " 'negSamplesNum',\n",
       " 'validExs',\n",
       " 'featureLex',\n",
       " 'rng',\n",
       " 'negSamplingDistr',\n",
       " 'negSamplingCum',\n",
       " 'arg2Id',\n",
       " 'testExs',\n",
       " 'relationNum',\n",
       " 'trainExs',\n",
       " 'negSamplingDistrPower']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#indexedData # is an instance of OieData.DataSetManager\n",
    "vars(indexedData).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "1476 [ 0.00067751  0.00135501  0.00203252]\n",
      "1476 [0.0006775067750677507, 0.0006775067750677507, 0.0006775067750677507]\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print indexedData.negSamplesNum\n",
    "print len(indexedData.negSamplingCum), indexedData.negSamplingCum[:3]\n",
    "print len(indexedData.negSamplingDistr), indexedData.negSamplingDistr[:3]\n",
    "print indexedData.negSamplingDistrPower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<learning.OieData.MatrixDataSet instance at 0x7ffb39f29b00>\n",
      "<processing.OiePreprocessor.FeatureLexicon instance at 0x7ffb3a2312d8>\n"
     ]
    }
   ],
   "source": [
    "print indexedData.trainExs\n",
    "print indexedData.featureLex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inducer = ReconstructInducer(indexedData, goldStandard, rand, maxEpochs, learningRate,\n",
    "                                 batchSize, embedSize, lambdaL1, lambdaL2, optimization, modelName,\n",
    "                                 model, fixedSampling, extEmb, extendedReg,\n",
    "                                 frequentEval, alpha)\n",
    "\n",
    "\n",
    "\n",
    "inducer.learn()\n",
    "\n",
    "saveModel(inducer, inducer.modelName)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
